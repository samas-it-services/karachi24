{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Karachi24_twitter_scraper.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python_defaultSpec_1600381730184",
      "display_name": "Python 3.8.2 64-bit"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNdTFUOmBWM0",
        "colab_type": "text"
      },
      "source": [
        "# Twitter Pulse Checker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x39hXNFaJ4Q",
        "colab_type": "text"
      },
      "source": [
        "![preview](https://cdn.pixabay.com/photo/2013/06/07/09/53/twitter-117595_960_720.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDNM2RrQBapg",
        "colab_type": "text"
      },
      "source": [
        "This is a quick and dirty way to get a sense of what's trending on Twitter related to a particular Topic. For my use case, I am focusing on the city of Seattle but you can easily apply this to any topic.\n",
        "\n",
        "**Use the GPU for this notebook to speed things up:** select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\".\n",
        "\n",
        "The code in this notebook does the following things:\n",
        "\n",
        "\n",
        "*   Scrapes Tweets related to the Topic you are interested in.\n",
        "*   Extracts relevant Tags from the text (NER: Named Entity Recognition).\n",
        "*   Does Sentiment Analysis on those Tweets.\n",
        "*   Provides some visualizations in an interactive format to get a 'pulse' of what's happening.\n",
        "\n",
        "We use Tweepy to scrape Twitter data and Flair to do NER / Sentiment Analysis. We use Seaborn for visualizations and all of this is possible because of the wonderful, free and fast (with GPU) Google Colab.\n",
        "\n",
        "**A bit about NER (Named Entity Recognition)** \n",
        "\n",
        "This is the process of extracting labels form text. \n",
        "\n",
        "So, take an example sentence: 'George Washington went to Washington'. NER will allow us to extract labels such as Person for 'George Washington' and Location for 'Washington (state)'. It is one of the most common and useful applications in NLP and, using it, we can extract labels from Tweets and do analysis on them.\n",
        "\n",
        "**A bit about Sentiment Analysis** \n",
        "\n",
        "Most commonly, this is the process of getting a sense of whether some text is Positive or Negative. More generally, you can apply it to any label of your choosing (Spam/No Spam etc.).\n",
        "\n",
        "So, 'I hated this movie' would be classified as a negative statement but 'I loved this movie' would be classified as positive. Again - it is a very useful application as it allows us to get a sense of people's opinions about something (Twitter topics, Movie reviews etc). \n",
        "\n",
        "To learn more about these applications, check out the Flair Github homepage and Tutorials: https://github.com/zalandoresearch/flair\n",
        "\n",
        "\n",
        "Note: You will need Twitter API keys (and of course a Twitter account) to make this work. You can get those by signing up here: https://developer.twitter.com/en/apps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f9m2ucbDH8a",
        "colab_type": "text"
      },
      "source": [
        "To get up and running, we need to import a bunch of stuff and install Flair. Run through the next 3 cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGc4FbSqCJDg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "04f0bd2c-40aa-4713-a3d1-1b826da67288",
        "tags": []
      },
      "source": [
        "# import lots of stuff\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "% matplotlib inline\n",
        "\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "UsageError: Line magic function `%` not found.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YaIwapFC7Yi",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "# install Flair\n",
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n  Cloning https://github.com/flairNLP/flair.git to c:\\users\\v6\\appdata\\local\\temp\\pip-req-build-1kccsg81\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n    Preparing wheel metadata: started\n    Preparing wheel metadata: finished with status 'done'\nRequirement already satisfied, skipping upgrade: bpemb>=0.3.2 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (0.3.2)\nRequirement already satisfied, skipping upgrade: tqdm>=4.26.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (4.49.0)\nRequirement already satisfied, skipping upgrade: janome in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (0.4.0)\nRequirement already satisfied, skipping upgrade: gensim>=3.4.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (3.8.3)\nRequirement already satisfied, skipping upgrade: lxml in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (4.5.2)\nRequirement already satisfied, skipping upgrade: gdown in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (3.12.2)\nRequirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (3.3.2)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in c:\\users\\v6\\appdata\\roaming\\python\\python38\\site-packages (from flair==0.6.0.post1) (2.8.1)\nRequirement already satisfied, skipping upgrade: regex in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (2020.7.14)\nRequirement already satisfied, skipping upgrade: transformers>=3.0.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (3.1.0)\nRequirement already satisfied, skipping upgrade: langdetect in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (1.0.8)\nRequirement already satisfied, skipping upgrade: segtok>=1.5.7 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (1.5.10)\nRequirement already satisfied, skipping upgrade: pytest>=5.3.2 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (6.0.2)\nRequirement already satisfied, skipping upgrade: sqlitedict>=1.6.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (1.7.0)\nRequirement already satisfied, skipping upgrade: mpld3==0.3 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (0.3)\nRequirement already satisfied, skipping upgrade: torch>=1.1.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (1.7.0.dev20200916+cpu)\nRequirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (0.1.91)\nRequirement already satisfied, skipping upgrade: konoha<5.0.0,>=4.0.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (4.6.1)\nRequirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (0.23.2)\nRequirement already satisfied, skipping upgrade: ftfy in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (5.8)\nRequirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (0.2.4)\nRequirement already satisfied, skipping upgrade: tabulate in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (0.8.7)\nRequirement already satisfied, skipping upgrade: deprecated>=1.2.4 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from flair==0.6.0.post1) (1.2.10)\nRequirement already satisfied, skipping upgrade: requests in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from bpemb>=0.3.2->flair==0.6.0.post1) (2.24.0)\nRequirement already satisfied, skipping upgrade: numpy in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from bpemb>=0.3.2->flair==0.6.0.post1) (1.19.2)\nRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim>=3.4.0->flair==0.6.0.post1) (1.5.2)\nRequirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim>=3.4.0->flair==0.6.0.post1) (2.1.1)\nRequirement already satisfied, skipping upgrade: Cython==0.29.14 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gensim>=3.4.0->flair==0.6.0.post1) (0.29.14)\nRequirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\v6\\appdata\\roaming\\python\\python38\\site-packages (from gensim>=3.4.0->flair==0.6.0.post1) (1.15.0)\nRequirement already satisfied, skipping upgrade: filelock in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from gdown->flair==0.6.0.post1) (3.0.12)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.6.0.post1) (2.4.7)\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.6.0.post1) (0.10.0)\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.6.0.post1) (1.2.0)\nRequirement already satisfied, skipping upgrade: pillow>=6.2.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.6.0.post1) (7.2.0)\nRequirement already satisfied, skipping upgrade: certifi>=2020.06.20 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib>=2.2.3->flair==0.6.0.post1) (2020.6.20)\nRequirement already satisfied, skipping upgrade: tokenizers==0.8.1.rc2 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.0->flair==0.6.0.post1) (0.8.1rc2)\nRequirement already satisfied, skipping upgrade: sacremoses in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.0->flair==0.6.0.post1) (0.0.43)\nRequirement already satisfied, skipping upgrade: packaging in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.0->flair==0.6.0.post1) (20.4)\nRequirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (8.5.0)\nRequirement already satisfied, skipping upgrade: attrs>=17.4.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (20.2.0)\nRequirement already satisfied, skipping upgrade: colorama; sys_platform == \"win32\" in c:\\users\\v6\\appdata\\roaming\\python\\python38\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (0.4.3)\nRequirement already satisfied, skipping upgrade: iniconfig in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (1.0.1)\nRequirement already satisfied, skipping upgrade: atomicwrites>=1.0; sys_platform == \"win32\" in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (1.4.0)\nRequirement already satisfied, skipping upgrade: py>=1.8.2 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (1.9.0)\nRequirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (0.13.1)\nRequirement already satisfied, skipping upgrade: toml in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pytest>=5.3.2->flair==0.6.0.post1) (0.10.1)\nRequirement already satisfied, skipping upgrade: future in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=1.1.0->flair==0.6.0.post1) (0.18.2)\nRequirement already satisfied, skipping upgrade: dataclasses in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=1.1.0->flair==0.6.0.post1) (0.6)\nRequirement already satisfied, skipping upgrade: typing-extensions in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=1.1.0->flair==0.6.0.post1) (3.7.4.3)\nRequirement already satisfied, skipping upgrade: overrides==3.0.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from konoha<5.0.0,>=4.0.0->flair==0.6.0.post1) (3.0.0)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=0.21.3->flair==0.6.0.post1) (0.16.0)\nRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=0.21.3->flair==0.6.0.post1) (2.1.0)\nRequirement already satisfied, skipping upgrade: wcwidth in c:\\users\\v6\\appdata\\roaming\\python\\python38\\site-packages (from ftfy->flair==0.6.0.post1) (0.2.5)\nRequirement already satisfied, skipping upgrade: cloudpickle in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hyperopt>=0.1.1->flair==0.6.0.post1) (1.6.0)\nRequirement already satisfied, skipping upgrade: networkx>=2.2 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from hyperopt>=0.1.1->flair==0.6.0.post1) (2.5)\nRequirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from deprecated>=1.2.4->flair==0.6.0.post1) (1.12.1)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.6.0.post1) (3.0.4)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.6.0.post1) (2.10)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->bpemb>=0.3.2->flair==0.6.0.post1) (1.25.10)\nRequirement already satisfied, skipping upgrade: boto in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->gensim>=3.4.0->flair==0.6.0.post1) (2.49.0)\nRequirement already satisfied, skipping upgrade: boto3 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smart-open>=1.8.1->gensim>=3.4.0->flair==0.6.0.post1) (1.14.63)\nRequirement already satisfied, skipping upgrade: click in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers>=3.0.0->flair==0.6.0.post1) (7.1.2)\nRequirement already satisfied, skipping upgrade: decorator>=4.3.0 in c:\\users\\v6\\appdata\\roaming\\python\\python38\\site-packages (from networkx>=2.2->hyperopt>=0.1.1->flair==0.6.0.post1) (4.4.2)\nRequirement already satisfied, skipping upgrade: botocore<1.18.0,>=1.17.63 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim>=3.4.0->flair==0.6.0.post1) (1.17.63)\nRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim>=3.4.0->flair==0.6.0.post1) (0.10.0)\nRequirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim>=3.4.0->flair==0.6.0.post1) (0.3.3)\nRequirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in c:\\users\\v6\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from botocore<1.18.0,>=1.17.63->boto3->smart-open>=1.8.1->gensim>=3.4.0->flair==0.6.0.post1) (0.15.2)\nBuilding wheels for collected packages: flair\n  Building wheel for flair (PEP 517): started\n  Building wheel for flair (PEP 517): finished with status 'done'\n  Created wheel for flair: filename=flair-0.6.0.post1-py3-none-any.whl size=202216 sha256=8a22bb7d23d423da71bae73be841606feb4855c1b179b087907a476a55f4b891\n  Stored in directory: C:\\Users\\V6\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-c377ncf2\\wheels\\06\\ac\\0f\\9e4e54611b479a240de1fadcdf1ebd44e19b7a5bfc9e27973c\nSuccessfully built flair\nInstalling collected packages: flair\n  Attempting uninstall: flair\n    Found existing installation: flair 0.6.0.post1\n    Uninstalling flair-0.6.0.post1:\n      Successfully uninstalled flair-0.6.0.post1\nSuccessfully installed flair-0.6.0.post1\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN7bPwceC77g",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "# import Flair stuff\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger = SequenceTagger.load('ner')\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhUwHI1zDDs_",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "#import Flair Classifier\n",
        "from flair.models import TextClassifier\n",
        "\n",
        "classifier = TextClassifier.load('en-sentiment')\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPfBYe-zqxme",
        "colab_type": "text"
      },
      "source": [
        "### Authenticate with Twitter API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D82o9BhxA0tq",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Enter Twitter Credentials\n",
        "TWITTER_KEY = 'HTi8RVQq3LczryTEyESQBdqNk' #@param {type:\"string\"}\n",
        "TWITTER_SECRET_KEY = '5VYQAWZTLuBs2p9mbLmkoGvCg7iR0vnXvKMdEDyEGcSKS5jcQk' #@param {type:\"string\"}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOxCv5dKBkVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate\n",
        "auth = tweepy.AppAuthHandler(TWITTER_KEY, TWITTER_SECRET_KEY)\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True,\n",
        "\t\t\t\t   wait_on_rate_limit_notify=True)\n",
        "\n",
        "if (not api):\n",
        "    print (\"Can't Authenticate\")\n",
        "    sys.exit(-1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0aiJl1VzKpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f39f7155-869a-401d-80d6-63bc27b36ec4"
      },
      "source": [
        "user = api.get_user('GillAtkinson11')\n",
        "user._json\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'id': 1151131792482553856,\n 'id_str': '1151131792482553856',\n 'name': 'Gill Atkinson',\n 'screen_name': 'GillAtkinson11',\n 'location': 'Abuja, Nigeria',\n 'profile_location': {'id': '00e55e2b4c491c5f',\n  'url': 'https://api.twitter.com/1.1/geo/id/00e55e2b4c491c5f.json',\n  'place_type': 'unknown',\n  'name': 'Abuja, Nigeria',\n  'full_name': 'Abuja, Nigeria',\n  'country_code': '',\n  'country': '',\n  'contained_within': [],\n  'bounding_box': None,\n  'attributes': {}},\n 'description': 'British Deputy High Commissioner in Abuja (stuck in 🇬🇧 Still ❤️🇳🇬 and will be back)',\n 'url': None,\n 'entities': {'description': {'urls': []}},\n 'protected': False,\n 'followers_count': 1196,\n 'friends_count': 612,\n 'listed_count': 7,\n 'created_at': 'Tue Jul 16 14:09:41 +0000 2019',\n 'favourites_count': 6315,\n 'utc_offset': None,\n 'time_zone': None,\n 'geo_enabled': False,\n 'verified': False,\n 'statuses_count': 1511,\n 'lang': None,\n 'status': {'created_at': 'Thu Sep 17 15:39:43 +0000 2020',\n  'id': 1306618845352861698,\n  'id_str': '1306618845352861698',\n  'text': 'Great to see my Mum looking happy in the sunshine after such a difficult year.  Wish I was with you all. https://t.co/sF3Ip2vLkD',\n  'truncated': False,\n  'entities': {'hashtags': [],\n   'symbols': [],\n   'user_mentions': [],\n   'urls': [{'url': 'https://t.co/sF3Ip2vLkD',\n     'expanded_url': 'https://twitter.com/growthemedian/status/1306588571072172037',\n     'display_url': 'twitter.com/growthemedian/…',\n     'indices': [105, 128]}]},\n  'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n  'in_reply_to_status_id': None,\n  'in_reply_to_status_id_str': None,\n  'in_reply_to_user_id': None,\n  'in_reply_to_user_id_str': None,\n  'in_reply_to_screen_name': None,\n  'geo': None,\n  'coordinates': None,\n  'place': None,\n  'contributors': None,\n  'is_quote_status': True,\n  'quoted_status_id': 1306588571072172037,\n  'quoted_status_id_str': '1306588571072172037',\n  'retweet_count': 1,\n  'favorite_count': 19,\n  'favorited': False,\n  'retweeted': False,\n  'possibly_sensitive': False,\n  'lang': 'en'},\n 'contributors_enabled': False,\n 'is_translator': False,\n 'is_translation_enabled': False,\n 'profile_background_color': 'F5F8FA',\n 'profile_background_image_url': None,\n 'profile_background_image_url_https': None,\n 'profile_background_tile': False,\n 'profile_image_url': 'http://pbs.twimg.com/profile_images/1151132305517244416/BtfAEIv4_normal.jpg',\n 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1151132305517244416/BtfAEIv4_normal.jpg',\n 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1151131792482553856/1572172245',\n 'profile_link_color': '1DA1F2',\n 'profile_sidebar_border_color': 'C0DEED',\n 'profile_sidebar_fill_color': 'DDEEF6',\n 'profile_text_color': '333333',\n 'profile_use_background_image': True,\n 'has_extended_profile': False,\n 'default_profile': True,\n 'default_profile_image': False,\n 'following': None,\n 'follow_request_sent': None,\n 'notifications': None,\n 'translator_type': 'none'}"
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0rweWLHXo1v",
        "colab_type": "text"
      },
      "source": [
        "###Lets start scraping!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8oyLAkVYp4k",
        "colab_type": "text"
      },
      "source": [
        "The Twitter scrape code here was taken from: https://bhaskarvk.github.io/2015/01/how-to-use-twitters-search-rest-api-most-effectively.\n",
        "\n",
        "My thanks to the author.\n",
        "\n",
        "We need to provide a Search term and a Max Tweet count. Twitter lets you to request 45,000 tweets every 15 minutes  so setting something below that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As_PRtb-Bklo",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "#@title Twitter Search API Inputs\n",
        "#@markdown ### Enter Search Query:\n",
        "searchQuery = 'Karachi' #@param {type:\"string\"}\n",
        "#@markdown ### Enter Max Tweets To Scrape:\n",
        "#@markdown #### The Twitter API Rate Limit (currently) is 45,000 tweets every 15 minutes.\n",
        "maxTweets = 100 #@param {type:\"slider\", min:0, max:45000, step:100}\n",
        "Filter_Retweets = True #@param {type:\"boolean\"}\n",
        "\n",
        "tweetsPerQry = 100  # this is the max the API permits\n",
        "tweet_lst = []\n",
        "\n",
        "if Filter_Retweets:\n",
        "  searchQuery = searchQuery + ' -filter:retweets'  # to exclude retweets\n",
        "\n",
        "# If results from a specific ID onwards are reqd, set since_id to that ID.\n",
        "# else default to no lower limit, go as far back as API allows\n",
        "sinceId = None\n",
        "\n",
        "# If results only below a specific ID are, set max_id to that ID.\n",
        "# else default to no upper limit, start from the most recent tweet matching the search query.\n",
        "max_id = -10000000000\n",
        "\n",
        "tweetCount = 0\n",
        "print(\"Downloading max {0} tweets\".format(maxTweets))\n",
        "while tweetCount < maxTweets:\n",
        "    try:\n",
        "        if (max_id <= 0):\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry, lang=\"en\")\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", since_id=sinceId)\n",
        "        else:\n",
        "            if (not sinceId):\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1))\n",
        "            else:\n",
        "                new_tweets = api.search(q=searchQuery, count=tweetsPerQry,\n",
        "                                        lang=\"en\", max_id=str(max_id - 1),\n",
        "                                        since_id=sinceId)\n",
        "        if not new_tweets:\n",
        "            print(\"No more tweets found\")\n",
        "            break\n",
        "        for tweet in new_tweets:\n",
        "          if hasattr(tweet, 'reply_count'):\n",
        "            reply_count = tweet.reply_count\n",
        "          else:\n",
        "            reply_count = 0\n",
        "          if hasattr(tweet, 'retweeted'):\n",
        "            retweeted = tweet.retweeted\n",
        "          else:\n",
        "            retweeted = \"NA\"\n",
        "            \n",
        "          # fixup search query to get topic\n",
        "          topic = searchQuery[:searchQuery.find('-')].capitalize().strip()\n",
        "          \n",
        "          # fixup date\n",
        "          tweetDate = tweet.created_at.date()\n",
        "\n",
        "    \n",
        "          \n",
        "          tweet_lst.append([tweetDate, topic, \n",
        "                      tweet.id, tweet.user.screen_name, tweet.user.name, tweet.user.followers_count, tweet.text, tweet.favorite_count, \n",
        "                    reply_count, tweet.retweet_count, retweeted, tweet.user.created_at, tweet.user.verified, tweet.user.location, tweet.user.statuses_count])\n",
        "        \n",
        "        tweetCount += len(new_tweets)\n",
        "        print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "        max_id = new_tweets[-1].id\n",
        "    except tweepy.TweepError as e:\n",
        "        # Just exit if any error\n",
        "        print(\"some error : \" + str(e))\n",
        "        break\n",
        "\n",
        "\n",
        "print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading max 100 tweets\nDownloaded 100 tweets\nDownloaded 100 tweets\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVsHZlEroRQY",
        "colab_type": "text"
      },
      "source": [
        "##Data Sciencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC0Lz66Jn48L",
        "colab_type": "text"
      },
      "source": [
        "Let's load the tweet data into a Pandas Dataframe so we can do Data Science to it. \n",
        "\n",
        "The data is also saved down in a tweets.csv file in case you want to download it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu7qN8q6Bkn9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "bd179b4f-f08c-40c6-9155-40c280b69735"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# load it into a pandas dataframe\n",
        "tweet_df = pd.DataFrame(tweet_lst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'followers', 'tweet', 'like_count', 'reply_count', 'retweet_count', 'retweeted', 'account_start_date', 'verified_user', 'location', 'number_of_tweets'])\n",
        "\n",
        "tweet_df.head(10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "     tweet_dt    topic                   id        username  \\\n0  2020-09-17  Karachi  1306739732869795851     UnivalesCom   \n1  2020-09-17  Karachi  1306738129982889990    benishaan195   \n2  2020-09-17  Karachi  1306737197555482624   Diplomat_APAC   \n3  2020-09-17  Karachi  1306737172729470977      MaqsoodAsi   \n4  2020-09-17  Karachi  1306736836358926342     Tweeterist_   \n5  2020-09-17  Karachi  1306736541163634688  TariqAKhan1905   \n6  2020-09-17  Karachi  1306736316504305664   forster_keith   \n7  2020-09-17  Karachi  1306735921908379650   whatsufiawhat   \n8  2020-09-17  Karachi  1306734472918949895        am1halal   \n9  2020-09-17  Karachi  1306734325417865217    apniwebcompk   \n\n                 name  followers  \\\n0        Univales.com        189   \n1                Baji      12573   \n2        The Diplomat     166243   \n3         Maqsood Asi       4124   \n4          Tweeterist        556   \n5      Tariq Ali Khan       2860   \n6       keith forster        892   \n7               صوفیہ        381   \n8  papi and 69 others          6   \n9       Apni Web News        133   \n\n                                                                                                                                            tweet  \\\n0    When will @reportpemra wake up to take strict notice against Nida Yasir, the host of a morning show on ARY? She inv… https://t.co/JeTFRwZ80o   \n1                                           @HarounRashid2 They are also gone from Karachi Saddar, very few you see are cladded in Shalwar kameez   \n2    Recent anti-Shia demonstrations in Karachi raise uncomfortable questions about the possible involvement of the Paki… https://t.co/zm62W6LonL   \n3  What Role Does the State Play in #Pakistan ’s Anti-Shia Hysteria?\\n\\nRecent anti-Shia demonstrations in #Karachi rais… https://t.co/1EwQLlqZbp   \n4                                      #Karachi restaurants boycott delivery service @foodpanda_pk over commission policy https://t.co/so5c75N6sU   \n5    My comments irritates those who are running on government funding!!! Because that's why they are paid. I can't igno… https://t.co/NWm0QbEjoo   \n6   @JLcab74 @JOEPUBLIC20171 Khan will win it.\\nHe's made London the new Karachi and the appreciative postal voters will… https://t.co/frAM6HQ4Wi   \n7                                                                                             She has learned ✨ Karachi ✨ https://t.co/tiq6z6FRla   \n8     😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔karachi😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔i hate you😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔   \n9                         Rescue Operation in Karachi by PAK Army | Headlines 10 AM | 26 August 2020 | Express News | EN1 https://t.co/13oCb97WYF   \n\n   like_count  reply_count  retweet_count  retweeted  account_start_date  \\\n0           0            0              0      False 2019-10-28 05:04:09   \n1           0            0              0      False 2012-03-08 04:47:38   \n2           0            0              0      False 2009-05-14 04:01:35   \n3           0            0              1      False 2013-03-22 21:51:52   \n4           0            0              0      False 2018-02-08 11:54:03   \n5           0            0              0      False 2017-12-23 23:05:34   \n6           0            0              0      False 2013-11-25 18:29:27   \n7           0            0              0      False 2019-08-17 14:27:43   \n8           3            0              0      False 2020-05-31 19:47:26   \n9           0            0              0      False 2017-08-12 04:16:50   \n\n   verified_user                  location  number_of_tweets  \n0          False                                        2401  \n1          False      We are also humans!!            141878  \n2           True              Tokyo, Japan             83911  \n3          False               Oslo, Norge            253311  \n4          False                  Far Away             46138  \n5          False                                       62436  \n6          False                Lancashire             11870  \n7          False  in your head, rent free.              2631  \n8          False                                          88  \n9          False                  Pakistan            127065  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_dt</th>\n      <th>topic</th>\n      <th>id</th>\n      <th>username</th>\n      <th>name</th>\n      <th>followers</th>\n      <th>tweet</th>\n      <th>like_count</th>\n      <th>reply_count</th>\n      <th>retweet_count</th>\n      <th>retweeted</th>\n      <th>account_start_date</th>\n      <th>verified_user</th>\n      <th>location</th>\n      <th>number_of_tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306739732869795851</td>\n      <td>UnivalesCom</td>\n      <td>Univales.com</td>\n      <td>189</td>\n      <td>When will @reportpemra wake up to take strict notice against Nida Yasir, the host of a morning show on ARY? She inv… https://t.co/JeTFRwZ80o</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2019-10-28 05:04:09</td>\n      <td>False</td>\n      <td></td>\n      <td>2401</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306738129982889990</td>\n      <td>benishaan195</td>\n      <td>Baji</td>\n      <td>12573</td>\n      <td>@HarounRashid2 They are also gone from Karachi Saddar, very few you see are cladded in Shalwar kameez</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2012-03-08 04:47:38</td>\n      <td>False</td>\n      <td>We are also humans!!</td>\n      <td>141878</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306737197555482624</td>\n      <td>Diplomat_APAC</td>\n      <td>The Diplomat</td>\n      <td>166243</td>\n      <td>Recent anti-Shia demonstrations in Karachi raise uncomfortable questions about the possible involvement of the Paki… https://t.co/zm62W6LonL</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2009-05-14 04:01:35</td>\n      <td>True</td>\n      <td>Tokyo, Japan</td>\n      <td>83911</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306737172729470977</td>\n      <td>MaqsoodAsi</td>\n      <td>Maqsood Asi</td>\n      <td>4124</td>\n      <td>What Role Does the State Play in #Pakistan ’s Anti-Shia Hysteria?\\n\\nRecent anti-Shia demonstrations in #Karachi rais… https://t.co/1EwQLlqZbp</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2013-03-22 21:51:52</td>\n      <td>False</td>\n      <td>Oslo, Norge</td>\n      <td>253311</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306736836358926342</td>\n      <td>Tweeterist_</td>\n      <td>Tweeterist</td>\n      <td>556</td>\n      <td>#Karachi restaurants boycott delivery service @foodpanda_pk over commission policy https://t.co/so5c75N6sU</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2018-02-08 11:54:03</td>\n      <td>False</td>\n      <td>Far Away</td>\n      <td>46138</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306736541163634688</td>\n      <td>TariqAKhan1905</td>\n      <td>Tariq Ali Khan</td>\n      <td>2860</td>\n      <td>My comments irritates those who are running on government funding!!! Because that's why they are paid. I can't igno… https://t.co/NWm0QbEjoo</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2017-12-23 23:05:34</td>\n      <td>False</td>\n      <td></td>\n      <td>62436</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306736316504305664</td>\n      <td>forster_keith</td>\n      <td>keith forster</td>\n      <td>892</td>\n      <td>@JLcab74 @JOEPUBLIC20171 Khan will win it.\\nHe's made London the new Karachi and the appreciative postal voters will… https://t.co/frAM6HQ4Wi</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2013-11-25 18:29:27</td>\n      <td>False</td>\n      <td>Lancashire</td>\n      <td>11870</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306735921908379650</td>\n      <td>whatsufiawhat</td>\n      <td>صوفیہ</td>\n      <td>381</td>\n      <td>She has learned ✨ Karachi ✨ https://t.co/tiq6z6FRla</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2019-08-17 14:27:43</td>\n      <td>False</td>\n      <td>in your head, rent free.</td>\n      <td>2631</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306734472918949895</td>\n      <td>am1halal</td>\n      <td>papi and 69 others</td>\n      <td>6</td>\n      <td>😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔karachi😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔i hate you😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔😔</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2020-05-31 19:47:26</td>\n      <td>False</td>\n      <td></td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2020-09-17</td>\n      <td>Karachi</td>\n      <td>1306734325417865217</td>\n      <td>apniwebcompk</td>\n      <td>Apni Web News</td>\n      <td>133</td>\n      <td>Rescue Operation in Karachi by PAK Army | Headlines 10 AM | 26 August 2020 | Express News | EN1 https://t.co/13oCb97WYF</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>2017-08-12 04:16:50</td>\n      <td>False</td>\n      <td>Pakistan</td>\n      <td>127065</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lJ8UlW3ZIsH",
        "colab_type": "text"
      },
      "source": [
        "Unfortunately Twitter does not let you filter by date when you request tweets. However, we can do this at this stage. I have set it up to pull yesterday + todays Tweets by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf_cZXTHBkqC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "021df2b9-fd66-452f-a077-14ef03f5fe0a"
      },
      "source": [
        "#@title Filter By Date Range\n",
        "today = datetime.now().date()\n",
        "yesterday = today - timedelta(1)\n",
        "\n",
        "start_dt = '2020-09-01' #@param {type:\"date\"}\n",
        "end_dt = '2020-09-14' #@param {type:\"date\"}\n",
        "\n",
        "if start_dt == '':\n",
        "  start_dt = yesterday\n",
        "else:\n",
        "  start_dt = datetime.strptime(start_dt, '%Y-%m-%d').date()\n",
        "\n",
        "if end_dt == '':\n",
        "  end_dt = today\n",
        "else:\n",
        "  end_dt = datetime.strptime(end_dt, '%Y-%m-%d').date()\n",
        "\n",
        "\n",
        "tweet_df = tweet_df[(tweet_df['tweet_dt'] >= start_dt) \n",
        "                    & (tweet_df['tweet_dt'] <= end_dt)]\n",
        "tweet_df.shape\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(0, 15)"
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC-cQNXwafbt",
        "colab_type": "text"
      },
      "source": [
        "## NER and Sentiment Analysis\n",
        "\n",
        "Now let's do some NER / Sentiment Analysis. We will use the Flair library: https://github.com/zalandoresearch/flair\n",
        "\n",
        "###NER\n",
        "\n",
        "Previosuly, we extracted, and then appended the Tags as separate rows in our dataframe. This helps us later on to Group by Tags.\n",
        "\n",
        "We also create a new 'Hashtag' Tag as Flair does not recognize it and it's a big one in this context.\n",
        "\n",
        "### Sentiment Analysis\n",
        "\n",
        "We use the Flair Classifier to get Polarity and Result and add those fields to our dataframe.\n",
        "\n",
        "**Warning:** This can be slow if you have lots of tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOKbfZlzBksW",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "# predict NER\n",
        "nerlst = []\n",
        "\n",
        "for index, row in tqdm(tweet_df.iterrows(), total=tweet_df.shape[0]):\n",
        "  cleanedTweet = row['tweet'].replace(\"#\", \"\")\n",
        "  sentence = Sentence(cleanedTweet, use_tokenizer=True)\n",
        "  \n",
        "  # predict NER tags\n",
        "  tagger.predict(sentence)\n",
        "\n",
        "  # get ner\n",
        "  ners = sentence.to_dict(tag_type='ner')['entities']\n",
        "  \n",
        "  # predict sentiment\n",
        "  classifier.predict(sentence)\n",
        "  \n",
        "  label = sentence.labels[0]\n",
        "  response = {'result': label.value, 'polarity':label.score}\n",
        "  \n",
        "  # get hashtags\n",
        "  hashtags = re.findall(r'#\\w+', row['tweet'])\n",
        "  if len(hashtags) >= 1:\n",
        "    for hashtag in hashtags:\n",
        "      ners.append({ 'type': 'Hashtag', 'text': hashtag })\n",
        "  \n",
        "  for ner in ners:\n",
        "    adj_polarity = response['polarity']\n",
        "    if response['result'] == 'NEGATIVE':\n",
        "      adj_polarity = response['polarity'] * -1\n",
        "    try:\n",
        "      ner['type']\n",
        "    except:\n",
        "      ner['type'] = ''      \n",
        "    nerlst.append([ row['tweet_dt'], row['topic'], row['id'], row['username'], \n",
        "                   row['name'], row['followers'], row['tweet'], ner['type'], ner['text'], response['result'], \n",
        "                   response['polarity'], adj_polarity, row['like_count'], row['reply_count'], \n",
        "                  row['retweet_count'], row['account_start_date'], row['verified_user'], row['location'], row['number_of_tweets'] ])\n",
        "\n",
        "clear_output()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETnIczIIyN_B",
        "colab_type": "text"
      },
      "source": [
        "Let's filter out obvious tags like #Seattle that would show up for this search. You can comment this portion out or use different Tags for your list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfZVjXldBkuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_ner = pd.DataFrame(nerlst, columns=['tweet_dt', 'topic', 'id', 'username', 'name', 'followers', 'tweet', 'tag_type', 'tag', 'sentiment', 'polarity', \n",
        "                                      'adj_polarity', 'like_count', 'reply_count', 'retweet_count', 'account_start_date', 'verified_user', 'location', 'number_of_tweets'])\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTryVIM3WCnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def days_before(date):\n",
        "  if date and date != 0 or date != '0':\n",
        "      date = str(date)\n",
        "      date = date.split()[0]\n",
        "      now = str(datetime.now())\n",
        "      now = now.split()[0]\n",
        "      d1 = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "      d2 = datetime.strptime(now, \"%Y-%m-%d\")\n",
        "      return abs((d2 - d1).days)\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA4KzHONyjE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "7627b49c-95f6-4722-b6d2-bf1fe880c198"
      },
      "source": [
        "#This cell does main data analysis\n",
        "df_ner_1 = df_ner\n",
        "#Removes all the duplicate tweets\n",
        "df_ner_1 = df_ner_1.drop_duplicates(subset = 'id')\n",
        "df_ner_1 = df_ner_1.reset_index(drop = True)\n",
        "del df_ner_1['polarity']\n",
        "df_ner_1['mean polarity'] = df_ner_1['adj_polarity'].mean()\n",
        "df_ner_1['polarity standard deviation'] = df_ner_1['adj_polarity'].std(axis = 0, ddof = 0)\n",
        "df_ner_1['average daily tweets'] = 0\n",
        "df_ner_1 = df_ner_1[['tweet_dt',\t'topic',\t'id',\t'username',\t'name',\t'followers',\t'tweet',\t'tag', 'tag_type',\t'average daily tweets', 'sentiment', 'adj_polarity',\t'mean polarity',\t'polarity standard deviation', 'like_count',\t'reply_count',\t'retweet_count', 'account_start_date', 'verified_user', 'location', 'number_of_tweets']]\n",
        "\n",
        "\n",
        "df_ner_1['account_start_date'] = df_ner_1['account_start_date'].apply(days_before)\n",
        "df_ner_1['average daily tweets'] = df_ner_1['number_of_tweets']/df_ner_1['account_start_date']\n",
        "del df_ner_1['account_start_date']\n",
        "\n",
        "df_ner_1.rename(columns = {'followers': 'number of followers', 'adj_polarity': 'polarity', 'number_of_tweets': 'total number of tweets'}, inplace = True)\n",
        "# df_ner_1 = df_ner_1.sort_values(by = 'average daily tweets', ascending=False)\n",
        "df_ner_1.tail()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Empty DataFrame\nColumns: [tweet_dt, topic, id, username, name, number of followers, tweet, tag, tag_type, average daily tweets, sentiment, polarity, mean polarity, polarity standard deviation, like_count, reply_count, retweet_count, verified_user, location, total number of tweets]\nIndex: []",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_dt</th>\n      <th>topic</th>\n      <th>id</th>\n      <th>username</th>\n      <th>name</th>\n      <th>number of followers</th>\n      <th>tweet</th>\n      <th>tag</th>\n      <th>tag_type</th>\n      <th>average daily tweets</th>\n      <th>sentiment</th>\n      <th>polarity</th>\n      <th>mean polarity</th>\n      <th>polarity standard deviation</th>\n      <th>like_count</th>\n      <th>reply_count</th>\n      <th>retweet_count</th>\n      <th>verified_user</th>\n      <th>location</th>\n      <th>total number of tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWYIsAvDAMUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saves as csv\n",
        "file_name = 'karachi24_data.csv'\n",
        "df_ner_1.to_csv(file_name, index = False)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzwXUKUwBkzM",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# filter out obvious tags\n",
        "banned_words = ['Seattle', 'WA', '#Seattle', '#seattle', 'Washington', 'SEATTLE', 'WASHINGTON',\n",
        "                'seattle', 'Seattle WA', 'seattle wa','Seattle, WA', 'Seattle WA USA', \n",
        "                'Seattle, Washington', 'Seattle Washington', 'Wa', 'wa', '#Wa',\n",
        "               '#wa', '#washington', '#Washington', '#WA', '#PNW', '#pnw', '#northwest']\n",
        "\n",
        "df_ner = df_ner[~df_ner['tag'].isin(banned_words)]"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajYB9VAC4-Ca",
        "colab_type": "text"
      },
      "source": [
        "Calculate Frequency, Likes, Replies, Retweets and Average Polarity per Tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA3E8UTwBkw6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "27369e70-1c71-482e-fe11-1051cde8905e"
      },
      "source": [
        "ner_groups = df_ner.groupby(['tag', 'tag_type']).agg({'tag': \"count\", 'adj_polarity': \"mean\", 'like_count': 'sum', 'reply_count': 'sum', 'retweet_count': 'sum'})\n",
        "ner_groups = ner_groups.rename(columns={\n",
        "   \"tag\": \"Frequency\",\n",
        "   \"adj_polarity\": \"Avg_Polarity\",\n",
        "   \"like_count\": \"Total_Likes\",\n",
        "   \"reply_count\": \"Total_Replies\",\n",
        "   \"retweet_count\": \"Total_Retweets\"\n",
        "})\n",
        "ner_groups = ner_groups.sort_values(['Frequency'], ascending  = False)\n",
        "ner_groups = ner_groups.reset_index()\n",
        "ner_groups['Polarity Standard Deviation'] = ner_groups['Avg_Polarity'].std()\n",
        "ner_groups = ner_groups[['tag',\t'tag_type',\t'Frequency',\t'Avg_Polarity',\t'Polarity Standard Deviation', 'Total_Likes',\t'Total_Replies', 'Total_Retweets']]\n",
        "ner_groups.head(10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DataError",
          "evalue": "No numeric types to aggregate",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mDataError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-40-7179df19cc04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mner_groups\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_ner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tag_type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'tag'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"count\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'adj_polarity'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'like_count'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reply_count'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'retweet_count'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'sum'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m ner_groups = ner_groups.rename(columns={\n\u001b[0;32m      3\u001b[0m    \u001b[1;34m\"tag\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Frequency\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m    \u001b[1;34m\"adj_polarity\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Avg_Polarity\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m    \u001b[1;34m\"like_count\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Total_Likes\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    947\u001b[0m             )\n\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_aggregate\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_agg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_agg_1dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_agg\u001b[1;34m(arg, func)\u001b[0m\n\u001b[0;32m    381\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_how\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m                     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magg_how\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_agg_1dim\u001b[1;34m(name, how, subset)\u001b[0m\n\u001b[0;32m    365\u001b[0m                         \u001b[1;34m\"nested dictionary is ambiguous in aggregation\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m                     )\n\u001b[1;32m--> 367\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcolg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0m_agg_2dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\groupby\\generic.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36mmean\u001b[1;34m(self, numeric_only)\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[0mName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \"\"\"\n\u001b[1;32m-> 1393\u001b[1;33m         return self._cython_agg_general(\n\u001b[0m\u001b[0;32m   1394\u001b[0m             \u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1395\u001b[0m             \u001b[0malt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[1;34m(self, how, alt, numeric_only, min_count)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No numeric types to aggregate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_aggregated_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mDataError\u001b[0m: No numeric types to aggregate"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inLWlkSh8IW_",
        "colab_type": "text"
      },
      "source": [
        "Create an overall Sentiment column based on the Average Polarity of the Tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeBq0NeO5H3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cbe93246-a8aa-4ffd-aec8-82f25b64e764"
      },
      "source": [
        "ner_groups['Sentiment'] = np.where(ner_groups['Avg_Polarity']>=0, 'POSITIVE', 'NEGATIVE')\n",
        "ner_groups.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ner_groups' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-41-b33352a978b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mner_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mner_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Avg_Polarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'POSITIVE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NEGATIVE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mner_groups\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'ner_groups' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dAMN0Y3S_gy",
        "colab_type": "text"
      },
      "source": [
        "Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9MkEUibTBu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = df_ner_1\n",
        "df2.drop(columns = ['sentiment', 'polarity', 'mean polarity', 'polarity standard deviation'])\n",
        "df2.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Empty DataFrame\nColumns: [tweet_dt, topic, id, username, name, number of followers, tweet, tag, tag_type, average daily tweets, sentiment, polarity, mean polarity, polarity standard deviation, like_count, reply_count, retweet_count, verified_user, location, total number of tweets]\nIndex: []",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_dt</th>\n      <th>topic</th>\n      <th>id</th>\n      <th>username</th>\n      <th>name</th>\n      <th>number of followers</th>\n      <th>tweet</th>\n      <th>tag</th>\n      <th>tag_type</th>\n      <th>average daily tweets</th>\n      <th>sentiment</th>\n      <th>polarity</th>\n      <th>mean polarity</th>\n      <th>polarity standard deviation</th>\n      <th>like_count</th>\n      <th>reply_count</th>\n      <th>retweet_count</th>\n      <th>verified_user</th>\n      <th>location</th>\n      <th>total number of tweets</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLzD6bwyauz9",
        "colab_type": "text"
      },
      "source": [
        "## Visualize!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbhxVDawfaEQ",
        "colab_type": "text"
      },
      "source": [
        "We can get some bar plots for the Tags based on the following metrics:\n",
        "\n",
        "\n",
        "\n",
        "*   Most Popular Tweets\n",
        "*   Most Liked Tweets\n",
        "*   Most Replied Tweets\n",
        "*   Most Retweeted Tweets\n",
        "\n",
        "By default, we do the analysis on all the Tags but we can also filter by Tag by checking the Filter_TAG box. \n",
        "This way we can further drill down into the metrics for Hashtags, Persons, Locations & Organizations.\n",
        "\n",
        "We cut the plots by Sentiment i.e. the color of the bars tells us if the overall Sentiment was Positive or Negative.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6lMQm32Bk1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "da466cac-1d2b-4568-b291-8d2caaf2618f"
      },
      "source": [
        "#@title Visualize Top TAGs\n",
        "Filter_TAG = False #@param {type:\"boolean\"}\n",
        "TAG = 'Location' #@param [\"Hashtag\", \"Person\", \"Location\", \"Organization\"]\n",
        "#@markdown ###Pick how many tags to display per chart:\n",
        "Top_N = 10 #@param {type:\"integer\"}\n",
        "\n",
        "# get TAG value\n",
        "if TAG != 'Hashtag':\n",
        "  TAG = TAG[:3].upper()\n",
        "\n",
        "if Filter_TAG:\n",
        "  filtered_group = ner_groups[(ner_groups['tag_type'] == TAG)]\n",
        "else:\n",
        "  filtered_group = ner_groups\n",
        "\n",
        "# plot the figures\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "fig.subplots_adjust(hspace=0.2, wspace=0.5)\n",
        "\n",
        "ax1 = fig.add_subplot(321)\n",
        "sns.barplot(x=\"Frequency\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax2 = fig.add_subplot(322)\n",
        "filtered_group = filtered_group.sort_values(['Total_Likes'], ascending=False)\n",
        "sns.barplot(x=\"Total_Likes\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax3 = fig.add_subplot(323)\n",
        "filtered_group = filtered_group.sort_values(['Total_Replies'], ascending=False)\n",
        "sns.barplot(x=\"Total_Replies\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "ax4 = fig.add_subplot(324)\n",
        "filtered_group = filtered_group.sort_values(['Total_Retweets'], ascending=False)\n",
        "sns.barplot(x=\"Total_Retweets\", y=\"tag\", data=filtered_group[:Top_N], hue=\"Sentiment\")\n",
        "\n",
        "ax1.title.set_text('Most Popular')\n",
        "ax2.title.set_text('Most Liked')\n",
        "ax3.title.set_text('Most Replied')\n",
        "ax4.title.set_text('Most Retweeted')\n",
        "\n",
        "ax1.set_ylabel('')    \n",
        "ax1.set_xlabel('')\n",
        "ax2.set_ylabel('')    \n",
        "ax2.set_xlabel('')\n",
        "ax3.set_ylabel('')    \n",
        "ax3.set_xlabel('')\n",
        "ax4.set_ylabel('')    \n",
        "ax4.set_xlabel('')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ner_groups' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-43-d83015904915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m   \u001b[0mfiltered_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mner_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTAG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m   \u001b[0mfiltered_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner_groups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# plot the figures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'ner_groups' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BybFabE9QyUv",
        "colab_type": "text"
      },
      "source": [
        "###Get the Average Polarity Distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRdgAEbsDLyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "c1495448-e3db-4cbc-8c90-f9c8a24ae956"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 6))\n",
        "sns.distplot(filtered_group['Avg_Polarity'], hist=False, kde_kws={\"shade\": True})"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_group' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-44-387c4200cc7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Avg_Polarity'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkde_kws\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"shade\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'filtered_group' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipaVFUOPiJrk",
        "colab_type": "text"
      },
      "source": [
        "## Word Cloud\n",
        "\n",
        "Let's build a Word Cloud based on these metrics. \n",
        "\n",
        "Since I am interested in Seattle, I am going to use overlay the Seattle city skyline view over my Word Cloud. \n",
        "You can change this by selecting a different Mask option from the drop down.\n",
        "\n",
        "Images for Masks can be found at:\n",
        "\n",
        "http://clipart-library.com/clipart/2099977.htm\n",
        "\n",
        "https://needpix.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfYNVV1upjbL",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "# download mask images\n",
        "!wget http://clipart-library.com/img/2099977.jpg -O seattle.jpg\n",
        "!wget https://storage.needpix.com/rsynced_images/trotting-horse-silhouette.jpg -O horse.jpg\n",
        "!wget https://storage.needpix.com/rsynced_images/black-balloon.jpg -O balloon.jpg\n",
        "  \n",
        "clear_output()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBdSsmLU8lh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de602202-76bb-4a8f-fcaa-30fe7fe0b052"
      },
      "source": [
        "#@title Build Word Cloud For Top TAGs\n",
        "Metric = 'Most Popular' #@param [\"Most Popular\", \"Most Liked\", \"Most Replied\", \"Most Retweeted\"]\n",
        "#@markdown\n",
        "Filter_TAG = False #@param {type:\"boolean\"}\n",
        "##@markdown\n",
        "TAG = 'Location' #@param [\"Hashtag\", \"Person\", \"Location\", \"Organization\"]\n",
        "Mask = 'Rectangle' #@param [\"Rectangle\", \"Seattle\", \"Balloon\", \"Horse\"]\n",
        "\n",
        "# get correct Metric value\n",
        "if Metric == 'Most Popular':\n",
        "   Metric = 'Frequency'\n",
        "elif Metric == 'Most Liked':\n",
        "   Metric = 'Total_Likes'\n",
        "elif Metric == 'Most Replied':\n",
        "   Metric = 'Total_Replies'\n",
        "elif Metric == 'Most Retweeted':\n",
        "   Metric = 'Total_Retweets'    \n",
        "\n",
        "# get TAG value\n",
        "if TAG != 'Hashtag':\n",
        "  TAG = TAG[:3].upper()\n",
        "\n",
        "if Filter_TAG:\n",
        "  filtered_group = ner_groups[(ner_groups['tag_type'] == TAG)]\n",
        "else:\n",
        "  filtered_group = ner_groups\n",
        "\n",
        "countDict = {}\n",
        "\n",
        "for index, row in filtered_group.iterrows():\n",
        "  if row[Metric] == 0:\n",
        "    row[Metric] = 1\n",
        "  countDict.update( {row['tag'] : row[Metric]} )\n",
        "  \n",
        "if Mask == 'Seattle':\n",
        "  Mask = np.array(Image.open(\"seattle.jpg\"))\n",
        "elif Mask == 'Rectangle':\n",
        "  Mask = np.array(Image.new('RGB', (800,600), (0, 0, 0)))\n",
        "elif Mask == 'Horse':\n",
        "  Mask = np.array(Image.open(\"horse.png\"))\n",
        "elif Mask == 'Balloon':\n",
        "  Mask = np.array(Image.open(\"balloon.jpg\"))\n",
        "\n",
        "clear_output()\n",
        "\n",
        "# Generate Word Cloud\n",
        "wordcloud = WordCloud(\n",
        "    max_words=100,\n",
        "#     max_font_size=50,\n",
        "    height=300,\n",
        "    width=800,\n",
        "    background_color = 'white',\n",
        "    mask=Mask,\n",
        "    contour_width=1,\n",
        "    contour_color='steelblue',\n",
        "    stopwords = STOPWORDS).generate_from_frequencies(countDict)\n",
        "fig = plt.figure(\n",
        "    figsize = (18, 18),\n",
        "    )\n",
        "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ner_groups' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-46-03135d1d4067>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[0mfiltered_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mner_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTAG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[0mfiltered_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mner_groups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mcountDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'ner_groups' is not defined"
          ]
        }
      ]
    }
  ]
}